{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка данных"
      ],
      "metadata": {
        "id": "uhqnanpq_30E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 19WJRvdFZgV1hxmVl52HCKq0gieTQk_uY #flats_checks_raster.csv\n",
        "!gdown 1tbxE-SZO5R37TiBscVZ2zbTGeTAIsUYk #таргет price трейн\n",
        "!gdown 1ZI-JsZ-6QxtHJKnxj1Mggscxumodxm64 # таргет price тест"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRnZVhGuRGFS",
        "outputId": "fb89ab69-5057-458f-af64-ad498d448147"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=19WJRvdFZgV1hxmVl52HCKq0gieTQk_uY\n",
            "From (redirected): https://drive.google.com/uc?id=19WJRvdFZgV1hxmVl52HCKq0gieTQk_uY&confirm=t&uuid=4da11b1c-5b06-4d80-9173-064b626dc548\n",
            "To: /content/flats_checks_raster.csv\n",
            "100% 1.05G/1.05G [00:05<00:00, 202MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tbxE-SZO5R37TiBscVZ2zbTGeTAIsUYk\n",
            "To: /content/y_train_msk_merged_2.pkl\n",
            "100% 788k/788k [00:00<00:00, 24.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZI-JsZ-6QxtHJKnxj1Mggscxumodxm64\n",
            "To: /content/y_test_msk_merged_2.pkl\n",
            "100% 140k/140k [00:00<00:00, 83.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install catboost\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from torch_geometric.nn import SAGEConv, BatchNorm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkDDoEavMf5q",
        "outputId": "f7663468-d206-41b0-a4b2-2bf6af265dfe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Предобработка датасетов. Выделение датасетов из единого"
      ],
      "metadata": {
        "id": "fVB65o_l_6c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# 1. Загружаем и ставим индекс. Это требуется потому, что индексы слетели из-за сокращения датасета после добавления данных ФНС\n",
        "df_mega_base = pd.read_csv('flats_checks_raster.csv', index_col='Unnamed: 0')\n",
        "\n",
        "# 1.1. Загружаем y_train и y_test. Таргеты были отдельно\n",
        "y_train = pd.read_pickle('y_train_msk_merged_2.pkl')\n",
        "y_test = pd.read_pickle('y_test_msk_merged_2.pkl')\n",
        "\n",
        "# 2. Объединяем их в один DataFrame\n",
        "y_full = pd.concat([y_train, y_test])\n",
        "\n",
        "# 3. Присоединяем таргеты к df_mega_base по индексу\n",
        "df_mega_base = df_mega_base.join(y_full, how='left')\n",
        "df_mega_base = df_mega_base.reset_index(drop=True)\n",
        "\n",
        "# 2. Удаляем все столбцы с 'emb' в названии. Удаляем эмбеддинги Славы, т.к. датасет содержал эмбеддинги tabfpn, мне они не трубуются\n",
        "df_mega_base = df_mega_base.loc[:, ~df_mega_base.columns.str.contains('emb')]\n",
        "\n",
        "# 3. Удаляем столбцы\n",
        "df_mega_base = df_mega_base.drop([\n",
        "    'TruncatedAverageBill', 'MedianBill',\n",
        "    'lat', 'lng', 'geometry', 'index_right',\n",
        "    'coordinates', 'polygon', 'district_id'\n",
        "], axis=1, errors='ignore')"
      ],
      "metadata": {
        "id": "ab4-tq1-MrFX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# заполняем пропуски средним\n",
        "for column in df_mega_base.columns:\n",
        "    df_mega_base[column].fillna(df_mega_base[column].mean(), inplace=True)\n",
        "\n",
        "# 4. Создаём df_check из столбцов ФНС. на нем будем валидироваться\n",
        "df_check = df_mega_base[[\n",
        "    'KktCount',\n",
        "    'AverageBill',\n",
        "    'CachePayPercent',\n",
        "    'IntensityOfNumberBills',\n",
        "    'RevenueIntensity',\n",
        "    'ReceiptTotalCount',\n",
        "    'price'\n",
        "]].copy()\n",
        "\n",
        "# 5. df_flats_rasters — это оставшиеся столбцы. Получаем датасет только из квартир + POI + растры\n",
        "df_flats_rasters = df_mega_base.drop(columns=df_check.columns, errors='ignore')\n",
        "\n",
        "\n",
        "# 6. Удаляем все столбцы, содержащие 'rast' в названии\n",
        "df_flats = df_flats_rasters.loc[:, ~df_flats_rasters.columns.str.contains('rast')]\n",
        "\n",
        "# 7. Делаем датасет, в котором есть только исходные данные по квартирам (ЦИАН)\n",
        "POI = [\n",
        "    'distance_to_center',\n",
        "    'highways_count',\n",
        "    'undergrounds_count',\n",
        "    'railways_count',\n",
        "    'time_to_metro',\n",
        "    'pop_dense',\n",
        "    'pop_work_dense',\n",
        "    'pop_child_dense',\n",
        "    'avg_age',\n",
        "    'trade_per_pers',\n",
        "    'pers_per_bed',\n",
        "    'self_goods_sold',\n",
        "    'other_goods_sold',\n",
        "    'org_num',\n",
        "    'entrep_num',\n",
        "    'buildings_apartments',\n",
        "    'buildings_service',\n",
        "    'buildings_retail',\n",
        "    'buildings_kindergarten',\n",
        "    'buildings_school',\n",
        "    'buildings_office',\n",
        "    'buildings_construction',\n",
        "    'buildings_commercial',\n",
        "    'buildings_hospital',\n",
        "    'buildings_university',\n",
        "    'buildings_public',\n",
        "    'buildings_industrial',\n",
        "    'buildings_church',\n",
        "    'education',\n",
        "    'food_buy',\n",
        "    'food_out',\n",
        "    'health',\n",
        "    'leisure',\n",
        "    'religion',\n",
        "    'services',\n",
        "    'shopping',\n",
        "    'transport',\n",
        "    'mun_district',\n",
        "]\n",
        "\n",
        "df_flats_without_POI = df_flats.drop(columns=POI, errors='ignore')\n",
        "df_flats_rasters_only = df_flats_rasters.drop(columns=POI, errors='ignore')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIfrAm7MoclU",
        "outputId": "acec6e9b-34f3-465f-b602-f317b2cf4cfc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-535601612.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_mega_base[column].fillna(df_mega_base[column].mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN предсказание на датасете квартир + растровой гео-информации"
      ],
      "metadata": {
        "id": "-xxRMvfkQ6GW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN\n",
        "Датасет квартиры МСК"
      ],
      "metadata": {
        "id": "X5owgBClCG6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. Загрузка и подготовка данных ===\n",
        "y = df_mega_base['price']\n",
        "X = df_flats_without_POI.copy()\n",
        "\n",
        "# Делим на train и test\n",
        "df_train_flats, df_test_flats, y_train_price, y_test_price = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=999\n",
        ")\n",
        "\n",
        "# Масштабируем таргеты\n",
        "target_scaler = StandardScaler()\n",
        "y_train_scaled = target_scaler.fit_transform(y_train_price.values.reshape(-1, 1))\n",
        "y_test_scaled = target_scaler.transform(y_test_price.values.reshape(-1, 1))\n",
        "\n",
        "# Объединение\n",
        "df_all = pd.concat([df_train_flats, df_test_flats], ignore_index=True)\n",
        "y_all = np.vstack([y_train_scaled, y_test_scaled])\n",
        "X_all = df_all.values.astype(np.float32)\n",
        "\n",
        "# === 2. Построение графа и PyG Data ===\n",
        "A = kneighbors_graph(X_all, n_neighbors=8, mode='connectivity', include_self=False)\n",
        "edge_index = torch.tensor(np.array(A.nonzero()), dtype=torch.long)"
      ],
      "metadata": {
        "id": "YLhhQDtiCIao"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === NEW: определим устройство ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = Data(\n",
        "    x=torch.tensor(X_all, dtype=torch.float),\n",
        "    edge_index=edge_index,\n",
        "    y=torch.tensor(y_all, dtype=torch.float)\n",
        ")\n",
        "\n",
        "# Маски\n",
        "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "train_mask[:len(df_train_flats)] = True\n",
        "test_mask[len(df_train_flats):] = True\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# Переносим data на GPU\n",
        "data = data.to(device)\n",
        "\n",
        "# === 3. GNN модель ===\n",
        "class DeepGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.4):\n",
        "        super(DeepGNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = BatchNorm(hidden_channels)\n",
        "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.bn3 = BatchNorm(hidden_channels)\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(hidden_channels // 2, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "    def get_embeddings(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "# Модель и оптимизатор\n",
        "model = DeepGNN(in_channels=X_all.shape[1], hidden_channels=192, out_channels=1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "6paL67GKCIgI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index).squeeze()\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask].squeeze())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# === 5. Оценка модели (RMSE на тесте) ===\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds_scaled = model(data.x, data.edge_index).squeeze()\n",
        "    preds_test_scaled = preds_scaled[data.test_mask].cpu().numpy()\n",
        "    y_test_scaled_true = data.y[data.test_mask].squeeze().cpu().numpy()\n",
        "\n",
        "    # Обратное масштабирование\n",
        "    preds_test = target_scaler.inverse_transform(preds_test_scaled.reshape(-1, 1))\n",
        "    y_test_true = target_scaler.inverse_transform(y_test_scaled_true.reshape(-1, 1))\n",
        "\n",
        "    # Вычисляем RMSE\n",
        "    rmse = mean_squared_error(y_test_true, preds_test) ** 0.5\n",
        "    print(f'\\nTest RMSE: {rmse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxQlK7EWCQMI",
        "outputId": "bc0f8bfc-df58-48c6-867d-1aaa48d982f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.1682\n",
            "Epoch 10, Loss: 1.0098\n",
            "Epoch 20, Loss: 1.0018\n",
            "Epoch 30, Loss: 0.9975\n",
            "Epoch 40, Loss: 0.9977\n",
            "Epoch 50, Loss: 0.9960\n",
            "Epoch 60, Loss: 0.9921\n",
            "Epoch 70, Loss: 0.9918\n",
            "Epoch 80, Loss: 0.9901\n",
            "Epoch 90, Loss: 0.9880\n",
            "\n",
            "Test RMSE: 19541928.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN\n",
        "Датасет квартиры МСК + POI"
      ],
      "metadata": {
        "id": "dOStEq6WCCpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. Загрузка и подготовка данных ===\n",
        "y = df_mega_base['price']\n",
        "X = df_flats.copy()\n",
        "\n",
        "# Делим на train и test\n",
        "df_train_flats, df_test_flats, y_train_price, y_test_price = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=999\n",
        ")\n",
        "\n",
        "# Масштабируем таргеты\n",
        "target_scaler = StandardScaler()\n",
        "y_train_scaled = target_scaler.fit_transform(y_train_price.values.reshape(-1, 1))\n",
        "y_test_scaled = target_scaler.transform(y_test_price.values.reshape(-1, 1))\n",
        "\n",
        "# Объединение\n",
        "df_all = pd.concat([df_train_flats, df_test_flats], ignore_index=True)\n",
        "y_all = np.vstack([y_train_scaled, y_test_scaled])\n",
        "X_all = df_all.values.astype(np.float32)\n",
        "\n",
        "# === 2. Построение графа и PyG Data ===\n",
        "A = kneighbors_graph(X_all, n_neighbors=8, mode='connectivity', include_self=False)\n",
        "edge_index = torch.tensor(np.array(A.nonzero()), dtype=torch.long)"
      ],
      "metadata": {
        "id": "CRdICM2IBqqE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === NEW: определим устройство ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = Data(\n",
        "    x=torch.tensor(X_all, dtype=torch.float),\n",
        "    edge_index=edge_index,\n",
        "    y=torch.tensor(y_all, dtype=torch.float)\n",
        ")\n",
        "\n",
        "# Маски\n",
        "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "train_mask[:len(df_train_flats)] = True\n",
        "test_mask[len(df_train_flats):] = True\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# Переносим data на GPU\n",
        "data = data.to(device)\n",
        "\n",
        "# === 3. GNN модель ===\n",
        "class DeepGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.4):\n",
        "        super(DeepGNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = BatchNorm(hidden_channels)\n",
        "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.bn3 = BatchNorm(hidden_channels)\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(hidden_channels // 2, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "    def get_embeddings(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "# Модель и оптимизатор\n",
        "model = DeepGNN(in_channels=X_all.shape[1], hidden_channels=192, out_channels=1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "UBxf4yM0Bqm5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index).squeeze()\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask].squeeze())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# === 5. Оценка модели (RMSE на тесте) ===\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds_scaled = model(data.x, data.edge_index).squeeze()\n",
        "    preds_test_scaled = preds_scaled[data.test_mask].cpu().numpy()\n",
        "    y_test_scaled_true = data.y[data.test_mask].squeeze().cpu().numpy()\n",
        "\n",
        "    # Обратное масштабирование\n",
        "    preds_test = target_scaler.inverse_transform(preds_test_scaled.reshape(-1, 1))\n",
        "    y_test_true = target_scaler.inverse_transform(y_test_scaled_true.reshape(-1, 1))\n",
        "\n",
        "    # Вычисляем RMSE\n",
        "    rmse = mean_squared_error(y_test_true, preds_test) ** 0.5\n",
        "    print(f'\\nTest RMSE: {rmse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1amWCsO4Bqcw",
        "outputId": "44d537d1-363f-4c04-8f8d-00e64cf70ffe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0436\n",
            "Epoch 10, Loss: 1.0023\n",
            "Epoch 20, Loss: 0.9976\n",
            "Epoch 30, Loss: 0.9951\n",
            "Epoch 40, Loss: 0.9940\n",
            "Epoch 50, Loss: 0.9917\n",
            "Epoch 60, Loss: 0.9843\n",
            "Epoch 70, Loss: 0.9788\n",
            "Epoch 80, Loss: 0.9707\n",
            "Epoch 90, Loss: 0.9628\n",
            "Epoch 100, Loss: 0.9530\n",
            "Epoch 110, Loss: 0.9396\n",
            "Epoch 120, Loss: 0.9246\n",
            "Epoch 130, Loss: 0.9205\n",
            "Epoch 140, Loss: 0.9113\n",
            "Epoch 150, Loss: 0.8944\n",
            "Epoch 160, Loss: 0.8918\n",
            "Epoch 170, Loss: 0.8798\n",
            "Epoch 180, Loss: 0.8618\n",
            "Epoch 190, Loss: 0.8599\n",
            "Epoch 200, Loss: 0.8481\n",
            "Epoch 210, Loss: 0.8431\n",
            "Epoch 220, Loss: 0.8294\n",
            "Epoch 230, Loss: 0.8176\n",
            "Epoch 240, Loss: 0.8121\n",
            "Epoch 250, Loss: 0.8070\n",
            "Epoch 260, Loss: 0.7970\n",
            "Epoch 270, Loss: 0.7839\n",
            "Epoch 280, Loss: 0.7827\n",
            "Epoch 290, Loss: 0.7707\n",
            "Epoch 300, Loss: 0.7619\n",
            "Epoch 310, Loss: 0.7491\n",
            "Epoch 320, Loss: 0.7465\n",
            "Epoch 330, Loss: 0.7383\n",
            "Epoch 340, Loss: 0.7399\n",
            "Epoch 350, Loss: 0.7314\n",
            "Epoch 360, Loss: 0.7261\n",
            "Epoch 370, Loss: 0.7188\n",
            "Epoch 380, Loss: 0.7214\n",
            "Epoch 390, Loss: 0.7128\n",
            "Epoch 400, Loss: 0.7020\n",
            "Epoch 410, Loss: 0.6929\n",
            "Epoch 420, Loss: 0.6914\n",
            "Epoch 430, Loss: 0.6818\n",
            "Epoch 440, Loss: 0.6923\n",
            "Epoch 450, Loss: 0.6919\n",
            "Epoch 460, Loss: 0.6730\n",
            "Epoch 470, Loss: 0.6929\n",
            "Epoch 480, Loss: 0.6724\n",
            "Epoch 490, Loss: 0.6667\n",
            "\n",
            "Test RMSE: 22268694.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN\n",
        "Датасет квартиры МСК + растровые данные"
      ],
      "metadata": {
        "id": "_3fjVoGgPoNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. Загрузка и подготовка данных ===\n",
        "y = df_mega_base['price']\n",
        "X = df_flats_rasters\n",
        "\n",
        "# Делим на train и test\n",
        "df_train_flats, df_test_flats, y_train_price, y_test_price = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=999\n",
        ")\n",
        "\n",
        "# Масштабируем таргеты\n",
        "target_scaler = StandardScaler()\n",
        "y_train_scaled = target_scaler.fit_transform(y_train_price.values.reshape(-1, 1))\n",
        "y_test_scaled = target_scaler.transform(y_test_price.values.reshape(-1, 1))\n",
        "\n",
        "# Объединение\n",
        "df_all = pd.concat([df_train_flats, df_test_flats], ignore_index=True)\n",
        "y_all = np.vstack([y_train_scaled, y_test_scaled])\n",
        "X_all = df_all.values.astype(np.float32)\n",
        "\n",
        "# === 2. Построение графа и PyG Data ===\n",
        "A = kneighbors_graph(X_all, n_neighbors=8, mode='connectivity', include_self=False)\n",
        "edge_index = torch.tensor(np.array(A.nonzero()), dtype=torch.long)"
      ],
      "metadata": {
        "id": "-nljJGskPh0M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === NEW: определим устройство ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = Data(\n",
        "    x=torch.tensor(X_all, dtype=torch.float),\n",
        "    edge_index=edge_index,\n",
        "    y=torch.tensor(y_all, dtype=torch.float)\n",
        ")\n",
        "\n",
        "# Маски\n",
        "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "train_mask[:len(df_train_flats)] = True\n",
        "test_mask[len(df_train_flats):] = True\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# Переносим data на GPU\n",
        "data = data.to(device)\n",
        "\n",
        "# === 3. GNN модель ===\n",
        "class DeepGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.4):\n",
        "        super(DeepGNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = BatchNorm(hidden_channels)\n",
        "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.bn3 = BatchNorm(hidden_channels)\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(hidden_channels // 2, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "    def get_embeddings(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "# Модель и оптимизатор\n",
        "model = DeepGNN(in_channels=X_all.shape[1], hidden_channels=192, out_channels=1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "bBEVeBOTPhxH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index).squeeze()\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask].squeeze())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# === 5. Оценка модели (RMSE на тесте) ===\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds_scaled = model(data.x, data.edge_index).squeeze()\n",
        "    preds_test_scaled = preds_scaled[data.test_mask].cpu().numpy()\n",
        "    y_test_scaled_true = data.y[data.test_mask].squeeze().cpu().numpy()\n",
        "\n",
        "    # Обратное масштабирование\n",
        "    preds_test = target_scaler.inverse_transform(preds_test_scaled.reshape(-1, 1))\n",
        "    y_test_true = target_scaler.inverse_transform(y_test_scaled_true.reshape(-1, 1))\n",
        "\n",
        "    # Вычисляем RMSE\n",
        "    rmse = mean_squared_error(y_test_true, preds_test) ** 0.5\n",
        "    print(f'\\nTest RMSE: {rmse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVSQiFvRPhuT",
        "outputId": "a8aa9ce6-6873-49b5-ac69-1f693984c806"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0558\n",
            "Epoch 10, Loss: 0.7734\n",
            "Epoch 20, Loss: 0.6786\n",
            "Epoch 30, Loss: 0.6056\n",
            "Epoch 40, Loss: 0.5434\n",
            "Epoch 50, Loss: 0.4949\n",
            "Epoch 60, Loss: 0.4400\n",
            "Epoch 70, Loss: 0.4058\n",
            "Epoch 80, Loss: 0.3660\n",
            "Epoch 90, Loss: 0.3350\n",
            "Epoch 100, Loss: 0.3080\n",
            "Epoch 110, Loss: 0.2862\n",
            "Epoch 120, Loss: 0.2734\n",
            "Epoch 130, Loss: 0.2620\n",
            "Epoch 140, Loss: 0.2510\n",
            "Epoch 150, Loss: 0.2389\n",
            "Epoch 160, Loss: 0.2285\n",
            "Epoch 170, Loss: 0.2154\n",
            "Epoch 180, Loss: 0.2099\n",
            "Epoch 190, Loss: 0.2015\n",
            "Epoch 200, Loss: 0.1957\n",
            "Epoch 210, Loss: 0.1870\n",
            "Epoch 220, Loss: 0.1866\n",
            "Epoch 230, Loss: 0.1860\n",
            "Epoch 240, Loss: 0.1786\n",
            "Epoch 250, Loss: 0.1696\n",
            "Epoch 260, Loss: 0.1689\n",
            "Epoch 270, Loss: 0.1642\n",
            "Epoch 280, Loss: 0.1566\n",
            "Epoch 290, Loss: 0.1550\n",
            "Epoch 300, Loss: 0.1511\n",
            "Epoch 310, Loss: 0.1470\n",
            "Epoch 320, Loss: 0.1452\n",
            "Epoch 330, Loss: 0.1417\n",
            "Epoch 340, Loss: 0.1402\n",
            "Epoch 350, Loss: 0.1384\n",
            "Epoch 360, Loss: 0.1343\n",
            "Epoch 370, Loss: 0.1321\n",
            "Epoch 380, Loss: 0.1304\n",
            "Epoch 390, Loss: 0.1278\n",
            "Epoch 400, Loss: 0.1325\n",
            "Epoch 410, Loss: 0.1235\n",
            "Epoch 420, Loss: 0.1204\n",
            "Epoch 430, Loss: 0.1197\n",
            "Epoch 440, Loss: 0.1171\n",
            "Epoch 450, Loss: 0.1172\n",
            "Epoch 460, Loss: 0.1145\n",
            "Epoch 470, Loss: 0.1176\n",
            "Epoch 480, Loss: 0.1114\n",
            "Epoch 490, Loss: 0.1080\n",
            "\n",
            "Test RMSE: 13947781.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN\n",
        "Датасет квартиры МСК + POI + растровые данные"
      ],
      "metadata": {
        "id": "POtCYJKdDMRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. Загрузка и подготовка данных ===\n",
        "y = df_mega_base['price']\n",
        "X = df_flats_rasters.copy()\n",
        "\n",
        "# Делим на train и test\n",
        "df_train_flats, df_test_flats, y_train_price, y_test_price = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=999\n",
        ")\n",
        "\n",
        "# Масштабируем таргеты\n",
        "target_scaler = StandardScaler()\n",
        "y_train_scaled = target_scaler.fit_transform(y_train_price.values.reshape(-1, 1))\n",
        "y_test_scaled = target_scaler.transform(y_test_price.values.reshape(-1, 1))\n",
        "\n",
        "# Объединение\n",
        "df_all = pd.concat([df_train_flats, df_test_flats], ignore_index=True)\n",
        "y_all = np.vstack([y_train_scaled, y_test_scaled])\n",
        "X_all = df_all.values.astype(np.float32)\n",
        "\n",
        "# === 2. Построение графа и PyG Data ===\n",
        "A = kneighbors_graph(X_all, n_neighbors=8, mode='connectivity', include_self=False)\n",
        "edge_index = torch.tensor(np.array(A.nonzero()), dtype=torch.long)"
      ],
      "metadata": {
        "id": "7xRqtCT1z8u1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === NEW: определим устройство ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = Data(\n",
        "    x=torch.tensor(X_all, dtype=torch.float),\n",
        "    edge_index=edge_index,\n",
        "    y=torch.tensor(y_all, dtype=torch.float)\n",
        ")\n",
        "\n",
        "# Маски\n",
        "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "train_mask[:len(df_train_flats)] = True\n",
        "test_mask[len(df_train_flats):] = True\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# Переносим data на GPU\n",
        "data = data.to(device)\n",
        "\n",
        "# === 3. GNN модель ===\n",
        "class DeepGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.4):\n",
        "        super(DeepGNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = BatchNorm(hidden_channels)\n",
        "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.bn3 = BatchNorm(hidden_channels)\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(hidden_channels // 2, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "    def get_embeddings(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "# Модель и оптимизатор\n",
        "model = DeepGNN(in_channels=X_all.shape[1], hidden_channels=192, out_channels=1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "ovvO1re81BGe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index).squeeze()\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask].squeeze())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# === 5. Оценка модели (RMSE на тесте) ===\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds_scaled = model(data.x, data.edge_index).squeeze()\n",
        "    preds_test_scaled = preds_scaled[data.test_mask].cpu().numpy()\n",
        "    y_test_scaled_true = data.y[data.test_mask].squeeze().cpu().numpy()\n",
        "\n",
        "    # Обратное масштабирование\n",
        "    preds_test = target_scaler.inverse_transform(preds_test_scaled.reshape(-1, 1))\n",
        "    y_test_true = target_scaler.inverse_transform(y_test_scaled_true.reshape(-1, 1))\n",
        "\n",
        "    # Вычисляем RMSE\n",
        "    rmse = mean_squared_error(y_test_true, preds_test) ** 0.5\n",
        "    print(f'\\nTest RMSE: {rmse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv7z9l8p1FBn",
        "outputId": "2fb93213-652a-46bb-f2e0-8c77d5ae35e6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0568\n",
            "Epoch 10, Loss: 0.7676\n",
            "Epoch 20, Loss: 0.6778\n",
            "Epoch 30, Loss: 0.6063\n",
            "Epoch 40, Loss: 0.5458\n",
            "Epoch 50, Loss: 0.4881\n",
            "Epoch 60, Loss: 0.4459\n",
            "Epoch 70, Loss: 0.4032\n",
            "Epoch 80, Loss: 0.3636\n",
            "Epoch 90, Loss: 0.3351\n",
            "Epoch 100, Loss: 0.3149\n",
            "Epoch 110, Loss: 0.2913\n",
            "Epoch 120, Loss: 0.2773\n",
            "Epoch 130, Loss: 0.2531\n",
            "Epoch 140, Loss: 0.2438\n",
            "Epoch 150, Loss: 0.2361\n",
            "Epoch 160, Loss: 0.2225\n",
            "Epoch 170, Loss: 0.2130\n",
            "Epoch 180, Loss: 0.2039\n",
            "Epoch 190, Loss: 0.2007\n",
            "Epoch 200, Loss: 0.1914\n",
            "Epoch 210, Loss: 0.1860\n",
            "Epoch 220, Loss: 0.1802\n",
            "Epoch 230, Loss: 0.1746\n",
            "Epoch 240, Loss: 0.1698\n",
            "Epoch 250, Loss: 0.1679\n",
            "Epoch 260, Loss: 0.1638\n",
            "Epoch 270, Loss: 0.1588\n",
            "Epoch 280, Loss: 0.1583\n",
            "Epoch 290, Loss: 0.1558\n",
            "Epoch 300, Loss: 0.1507\n",
            "Epoch 310, Loss: 0.1462\n",
            "Epoch 320, Loss: 0.1437\n",
            "Epoch 330, Loss: 0.1466\n",
            "Epoch 340, Loss: 0.1376\n",
            "Epoch 350, Loss: 0.1399\n",
            "Epoch 360, Loss: 0.1318\n",
            "Epoch 370, Loss: 0.1355\n",
            "Epoch 380, Loss: 0.1286\n",
            "Epoch 390, Loss: 0.1276\n",
            "Epoch 400, Loss: 0.1243\n",
            "Epoch 410, Loss: 0.1232\n",
            "Epoch 420, Loss: 0.1232\n",
            "Epoch 430, Loss: 0.1227\n",
            "Epoch 440, Loss: 0.1178\n",
            "Epoch 450, Loss: 0.1151\n",
            "Epoch 460, Loss: 0.1150\n",
            "Epoch 470, Loss: 0.1116\n",
            "Epoch 480, Loss: 0.1118\n",
            "Epoch 490, Loss: 0.1107\n",
            "\n",
            "Test RMSE: 13954009.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это лучший результат, извлечем его эмбеддинги для дальнейшего использования"
      ],
      "metadata": {
        "id": "lX5C5eYIHL1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5. Извлечение эмбеддингов ===\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    embeddings = model.get_embeddings(data.x, data.edge_index).cpu().numpy()"
      ],
      "metadata": {
        "id": "JjjXr2vIRCzE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Создаем DataFrame с эмбеддингами и правильными индексами из X_all\n",
        "embeddings = pd.DataFrame(embeddings, index=df_mega_base.index)\n",
        "embeddings.to_csv('embeddings.csv')"
      ],
      "metadata": {
        "id": "fNElScMxHWxe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#подгружаем эмбеддинги, чтобы не пересчитывать\n",
        "#!gdown 1pZIcstb6kL90U8NfTpDKd7TOHr2FtEFC\n",
        "#embeddings = pd.read_csv('embeddings.csv')"
      ],
      "metadata": {
        "id": "5CPmu5JP_RAi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Валидация. Датасет с фискальными данными"
      ],
      "metadata": {
        "id": "VbKh4ATrG99I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предсказание catboost на сыром датасете"
      ],
      "metadata": {
        "id": "iKNtWJk98A6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# === 1. Берем df_check и определяем X и y ===\n",
        "target_col = 'AverageBill'\n",
        "X = df_check.drop(columns=[target_col])\n",
        "y = df_check[target_col]\n",
        "\n",
        "# === 2. Делим на train/test ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=999\n",
        ")\n",
        "\n",
        "# === 3. Масштабируем признаки по train ===\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# === 4. Обучение CatBoost ===\n",
        "model = CatBoostRegressor(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=0,\n",
        "    random_state=999\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# === 5. Предсказания и оценка RMSE ===\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "print(f'RMSE: {rmse:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyEabaL96YXo",
        "outputId": "49695d05-6f58-4ef6-a702-81de757b133d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 120.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предсказание catboost на датасете с добавленными эмбеддингами из GNN"
      ],
      "metadata": {
        "id": "UE8onZWa7_dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Создаем DataFrame с эмбеддингами и правильными индексами из X_all\n",
        "emb_df = pd.DataFrame(embeddings, index=df_mega_base.index)\n",
        "emb_df.columns = [f'emb_{i}' for i in range(embeddings.shape[1])]\n",
        "\n",
        "# 2. Присоединяем эмбеддинги по индексу\n",
        "existing_cols = set(df_check.columns)\n",
        "\n",
        "# 3. Отбираем только те столбцы из emb_df, которых нет в df_check\n",
        "emb_to_add = emb_df[[col for col in emb_df.columns if col not in existing_cols]]\n",
        "\n",
        "# 4. Присоединяем по индексу\n",
        "df_check_emb = df_check.join(emb_to_add, how='left')"
      ],
      "metadata": {
        "id": "M6TC_QCnDSFx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# === 1. Берем df_check и определяем X и y ===\n",
        "target_col = 'AverageBill'\n",
        "X = df_check_emb.drop(columns=[target_col])\n",
        "y = df_check_emb[target_col]\n",
        "\n",
        "# === 2. Делим на train/test ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=999\n",
        ")\n",
        "\n",
        "# === 3. Масштабируем признаки по train ===\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# === 4. Обучение CatBoost ===\n",
        "model = CatBoostRegressor(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=0,\n",
        "    random_state=999\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# === 5. Предсказания и оценка RMSE ===\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "print(f'RMSE: {rmse:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE5hWc9R4YKh",
        "outputId": "aeb7c2e5-b725-4635-fd80-b01eded099a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 133.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На датасете с фискальной информацией полученные GNN на квартирах эмбеддинги не дали улучшения метрики, а наоборот - ухудшили результаты. Видимо, они внесли шум, либо датасет квартир был недостаточно большим"
      ],
      "metadata": {
        "id": "u3whnLYs_4D5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nMD7qVZvAW9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}